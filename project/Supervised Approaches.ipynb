{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim \n",
    "# from gensim.models import Word2Vec\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>place</th>\n",
       "      <th>near</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemma</th>\n",
       "      <th>lemma_sentence</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>lemma_sentence(with POS)</th>\n",
       "      <th>sentiword_analysis</th>\n",
       "      <th>vader_score</th>\n",
       "      <th>textblob_polarity</th>\n",
       "      <th>senti_textblob</th>\n",
       "      <th>senti_wordnet</th>\n",
       "      <th>senti_vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>293175196</td>\n",
       "      <td>sjtafalla</td>\n",
       "      <td>UK Parliament: 2nd Covid Vaccine should be 21 ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>parliament covid vaccine days not weeks challe...</td>\n",
       "      <td>['parliament', 'covid', 'vaccine', 'days', 'no...</td>\n",
       "      <td>['parliament', 'covid', 'vaccine', 'day', 'not...</td>\n",
       "      <td>parliament covid vaccine day not week challeng...</td>\n",
       "      <td>[('parliament', 'n'), ('covid', 'n'), ('vaccin...</td>\n",
       "      <td>parliament covid vaccine day not week challe...</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>-0.0572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>1591779799</td>\n",
       "      <td>ellieelif</td>\n",
       "      <td>First dose of vaccination 💉 5/1/2021..to comba...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>first dose vaccination syringe combating covid</td>\n",
       "      <td>['first', 'dose', 'vaccination', 'syringe', 'c...</td>\n",
       "      <td>['first', 'dose', 'vaccination', 'syringe', 'c...</td>\n",
       "      <td>first dose vaccination syringe combating covid</td>\n",
       "      <td>[('first', 'r'), ('dose', 'a'), ('vaccination'...</td>\n",
       "      <td>first dose vaccination syringe combat covid</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.3400</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>215143656</td>\n",
       "      <td>danananarama</td>\n",
       "      <td>Time to forget about #COVID, #Brexit and #Trum...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>time forget covid brexit trump sleep listening...</td>\n",
       "      <td>['time', 'forget', 'covid', 'brexit', 'trump',...</td>\n",
       "      <td>['time', 'forget', 'covid', 'brexit', 'trump',...</td>\n",
       "      <td>time forget covid brexit trump sleep listening...</td>\n",
       "      <td>[('time', 'n'), ('forget', 'v'), ('covid', 'a'...</td>\n",
       "      <td>time forget covid brexit trump sleep listeni...</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.2263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>336462129</td>\n",
       "      <td>veronica_foote_</td>\n",
       "      <td>@doctor_oxford Rachel you absolutely nailed it...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>rachel absolutely nailed tonight throughout pr...</td>\n",
       "      <td>['rachel', 'absolutely', 'nailed', 'tonight', ...</td>\n",
       "      <td>['rachel', 'absolutely', 'nailed', 'tonight', ...</td>\n",
       "      <td>rachel absolutely nailed tonight throughout pr...</td>\n",
       "      <td>[('rachel', 'n'), ('absolutely', 'r'), ('naile...</td>\n",
       "      <td>rachel absolutely nail tonight throughout pr...</td>\n",
       "      <td>1.375</td>\n",
       "      <td>0.6124</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>1063705581133934593</td>\n",
       "      <td>5herii</td>\n",
       "      <td>My kids can never say they don’t wanna do thei...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>kids never say not wana homework got sparkles ...</td>\n",
       "      <td>['kids', 'never', 'say', 'not', 'wana', 'homew...</td>\n",
       "      <td>['kid', 'never', 'say', 'not', 'wana', 'homewo...</td>\n",
       "      <td>kid never say not wana homework got sparkle co...</td>\n",
       "      <td>[('kids', 'n'), ('never', 'r'), ('say', 'v'), ...</td>\n",
       "      <td>kid never say not wana homework get sparkle ...</td>\n",
       "      <td>-0.750</td>\n",
       "      <td>0.6573</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29929</th>\n",
       "      <td>29929</td>\n",
       "      <td>2021-01-30</td>\n",
       "      <td>1200125350694576128</td>\n",
       "      <td>MayorJesse</td>\n",
       "      <td>Health Rover will be offering rapid COVID-19 a...</td>\n",
       "      <td>{'type': 'Feature', 'bbox': [-72.4243, 40.8667...</td>\n",
       "      <td>Southampton, NY</td>\n",
       "      <td>health rover offering rapid covid antigen test...</td>\n",
       "      <td>['health', 'rover', 'offering', 'rapid', 'covi...</td>\n",
       "      <td>['health', 'rover', 'offering', 'rapid', 'covi...</td>\n",
       "      <td>health rover offering rapid covid antigen test...</td>\n",
       "      <td>[('health', 'n'), ('rover', 'n'), ('offering',...</td>\n",
       "      <td>health rover offering rapid covid antigen te...</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29930</th>\n",
       "      <td>29930</td>\n",
       "      <td>2021-01-27</td>\n",
       "      <td>803004340608843776</td>\n",
       "      <td>thatmikeny</td>\n",
       "      <td>#Americaorbust FOX complicit in so Manet death...</td>\n",
       "      <td>{'type': 'Feature', 'bbox': [-72.4243, 40.8667...</td>\n",
       "      <td>Southampton, NY</td>\n",
       "      <td>americaorbust fox complicit manet deaths due c...</td>\n",
       "      <td>['americaorbust', 'fox', 'complicit', 'manet',...</td>\n",
       "      <td>['americaorbust', 'fox', 'complicit', 'manet',...</td>\n",
       "      <td>americaorbust fox complicit manet death due co...</td>\n",
       "      <td>[('americaorbust', 'a'), ('fox', 'n'), ('compl...</td>\n",
       "      <td>americaorbust fox complicit manet death due ...</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>-0.8126</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29931</th>\n",
       "      <td>29931</td>\n",
       "      <td>2021-01-21</td>\n",
       "      <td>408755761</td>\n",
       "      <td>ConstanceHunter</td>\n",
       "      <td>Here is @POTUS plan to defeat #COVID19. 👇🏻\\nSt...</td>\n",
       "      <td>{'type': 'Feature', 'bbox': [-72.4243, 40.8667...</td>\n",
       "      <td>Southampton, NY</td>\n",
       "      <td>plan defeat covid backhand index pointing down...</td>\n",
       "      <td>['plan', 'defeat', 'covid', 'backhand', 'index...</td>\n",
       "      <td>['plan', 'defeat', 'covid', 'backhand', 'index...</td>\n",
       "      <td>plan defeat covid backhand index pointing down...</td>\n",
       "      <td>[('plan', 'n'), ('defeat', 'v'), ('covid', 'a'...</td>\n",
       "      <td>plan defeat covid backhand index point down ...</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.6369</td>\n",
       "      <td>0.122222</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29932</th>\n",
       "      <td>29932</td>\n",
       "      <td>2021-01-20</td>\n",
       "      <td>1247264861601619969</td>\n",
       "      <td>EBalabanidou</td>\n",
       "      <td>#morethanjustbones seems more relevant than ev...</td>\n",
       "      <td>{'type': 'Feature', 'bbox': [-1.43332383719785...</td>\n",
       "      <td>Southampton General Hospital</td>\n",
       "      <td>morethanjustbones seems more relevant ever fee...</td>\n",
       "      <td>['morethanjustbones', 'seems', 'more', 'releva...</td>\n",
       "      <td>['morethanjustbones', 'seems', 'more', 'releva...</td>\n",
       "      <td>morethanjustbones seems more relevant ever fee...</td>\n",
       "      <td>[('morethanjustbones', 'n'), ('seems', 'v'), (...</td>\n",
       "      <td>morethanjustbones seem more relevant ever fe...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.6597</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29933</th>\n",
       "      <td>29933</td>\n",
       "      <td>2021-01-19</td>\n",
       "      <td>1048277187059757057</td>\n",
       "      <td>SammiGardiner</td>\n",
       "      <td>Big thanks sent from @UHSFT to @dominos tonigh...</td>\n",
       "      <td>{'type': 'Feature', 'bbox': [-1.43332383719785...</td>\n",
       "      <td>Southampton General Hospital</td>\n",
       "      <td>big thanks sent tonight sending morale key wor...</td>\n",
       "      <td>['big', 'thanks', 'sent', 'tonight', 'sending'...</td>\n",
       "      <td>['big', 'thanks', 'sent', 'tonight', 'sending'...</td>\n",
       "      <td>big thanks sent tonight sending morale key wor...</td>\n",
       "      <td>[('big', 'a'), ('thanks', 'n'), ('sent', 'v'),...</td>\n",
       "      <td>big thanks send tonight send morale key work...</td>\n",
       "      <td>-1.250</td>\n",
       "      <td>0.4767</td>\n",
       "      <td>-0.018333</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29934 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  created_at              user_id         username  \\\n",
       "0               0  2021-01-06            293175196        sjtafalla   \n",
       "1               1  2021-01-06           1591779799        ellieelif   \n",
       "2               2  2021-01-06            215143656     danananarama   \n",
       "3               3  2021-01-06            336462129  veronica_foote_   \n",
       "4               4  2021-01-06  1063705581133934593           5herii   \n",
       "...           ...         ...                  ...              ...   \n",
       "29929       29929  2021-01-30  1200125350694576128       MayorJesse   \n",
       "29930       29930  2021-01-27   803004340608843776       thatmikeny   \n",
       "29931       29931  2021-01-21            408755761  ConstanceHunter   \n",
       "29932       29932  2021-01-20  1247264861601619969     EBalabanidou   \n",
       "29933       29933  2021-01-19  1048277187059757057    SammiGardiner   \n",
       "\n",
       "                                                   tweet  \\\n",
       "0      UK Parliament: 2nd Covid Vaccine should be 21 ...   \n",
       "1      First dose of vaccination 💉 5/1/2021..to comba...   \n",
       "2      Time to forget about #COVID, #Brexit and #Trum...   \n",
       "3      @doctor_oxford Rachel you absolutely nailed it...   \n",
       "4      My kids can never say they don’t wanna do thei...   \n",
       "...                                                  ...   \n",
       "29929  Health Rover will be offering rapid COVID-19 a...   \n",
       "29930  #Americaorbust FOX complicit in so Manet death...   \n",
       "29931  Here is @POTUS plan to defeat #COVID19. 👇🏻\\nSt...   \n",
       "29932  #morethanjustbones seems more relevant than ev...   \n",
       "29933  Big thanks sent from @UHSFT to @dominos tonigh...   \n",
       "\n",
       "                                                   place  \\\n",
       "0                                                    NaN   \n",
       "1                                                    NaN   \n",
       "2                                                    NaN   \n",
       "3                                                    NaN   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "29929  {'type': 'Feature', 'bbox': [-72.4243, 40.8667...   \n",
       "29930  {'type': 'Feature', 'bbox': [-72.4243, 40.8667...   \n",
       "29931  {'type': 'Feature', 'bbox': [-72.4243, 40.8667...   \n",
       "29932  {'type': 'Feature', 'bbox': [-1.43332383719785...   \n",
       "29933  {'type': 'Feature', 'bbox': [-1.43332383719785...   \n",
       "\n",
       "                               near  \\\n",
       "0                            London   \n",
       "1                            London   \n",
       "2                            London   \n",
       "3                            London   \n",
       "4                            London   \n",
       "...                             ...   \n",
       "29929               Southampton, NY   \n",
       "29930               Southampton, NY   \n",
       "29931               Southampton, NY   \n",
       "29932  Southampton General Hospital   \n",
       "29933  Southampton General Hospital   \n",
       "\n",
       "                                             clean_tweet  \\\n",
       "0      parliament covid vaccine days not weeks challe...   \n",
       "1         first dose vaccination syringe combating covid   \n",
       "2      time forget covid brexit trump sleep listening...   \n",
       "3      rachel absolutely nailed tonight throughout pr...   \n",
       "4      kids never say not wana homework got sparkles ...   \n",
       "...                                                  ...   \n",
       "29929  health rover offering rapid covid antigen test...   \n",
       "29930  americaorbust fox complicit manet deaths due c...   \n",
       "29931  plan defeat covid backhand index pointing down...   \n",
       "29932  morethanjustbones seems more relevant ever fee...   \n",
       "29933  big thanks sent tonight sending morale key wor...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0      ['parliament', 'covid', 'vaccine', 'days', 'no...   \n",
       "1      ['first', 'dose', 'vaccination', 'syringe', 'c...   \n",
       "2      ['time', 'forget', 'covid', 'brexit', 'trump',...   \n",
       "3      ['rachel', 'absolutely', 'nailed', 'tonight', ...   \n",
       "4      ['kids', 'never', 'say', 'not', 'wana', 'homew...   \n",
       "...                                                  ...   \n",
       "29929  ['health', 'rover', 'offering', 'rapid', 'covi...   \n",
       "29930  ['americaorbust', 'fox', 'complicit', 'manet',...   \n",
       "29931  ['plan', 'defeat', 'covid', 'backhand', 'index...   \n",
       "29932  ['morethanjustbones', 'seems', 'more', 'releva...   \n",
       "29933  ['big', 'thanks', 'sent', 'tonight', 'sending'...   \n",
       "\n",
       "                                                   lemma  \\\n",
       "0      ['parliament', 'covid', 'vaccine', 'day', 'not...   \n",
       "1      ['first', 'dose', 'vaccination', 'syringe', 'c...   \n",
       "2      ['time', 'forget', 'covid', 'brexit', 'trump',...   \n",
       "3      ['rachel', 'absolutely', 'nailed', 'tonight', ...   \n",
       "4      ['kid', 'never', 'say', 'not', 'wana', 'homewo...   \n",
       "...                                                  ...   \n",
       "29929  ['health', 'rover', 'offering', 'rapid', 'covi...   \n",
       "29930  ['americaorbust', 'fox', 'complicit', 'manet',...   \n",
       "29931  ['plan', 'defeat', 'covid', 'backhand', 'index...   \n",
       "29932  ['morethanjustbones', 'seems', 'more', 'releva...   \n",
       "29933  ['big', 'thanks', 'sent', 'tonight', 'sending'...   \n",
       "\n",
       "                                          lemma_sentence  \\\n",
       "0      parliament covid vaccine day not week challeng...   \n",
       "1         first dose vaccination syringe combating covid   \n",
       "2      time forget covid brexit trump sleep listening...   \n",
       "3      rachel absolutely nailed tonight throughout pr...   \n",
       "4      kid never say not wana homework got sparkle co...   \n",
       "...                                                  ...   \n",
       "29929  health rover offering rapid covid antigen test...   \n",
       "29930  americaorbust fox complicit manet death due co...   \n",
       "29931  plan defeat covid backhand index pointing down...   \n",
       "29932  morethanjustbones seems more relevant ever fee...   \n",
       "29933  big thanks sent tonight sending morale key wor...   \n",
       "\n",
       "                                                 pos_tag  \\\n",
       "0      [('parliament', 'n'), ('covid', 'n'), ('vaccin...   \n",
       "1      [('first', 'r'), ('dose', 'a'), ('vaccination'...   \n",
       "2      [('time', 'n'), ('forget', 'v'), ('covid', 'a'...   \n",
       "3      [('rachel', 'n'), ('absolutely', 'r'), ('naile...   \n",
       "4      [('kids', 'n'), ('never', 'r'), ('say', 'v'), ...   \n",
       "...                                                  ...   \n",
       "29929  [('health', 'n'), ('rover', 'n'), ('offering',...   \n",
       "29930  [('americaorbust', 'a'), ('fox', 'n'), ('compl...   \n",
       "29931  [('plan', 'n'), ('defeat', 'v'), ('covid', 'a'...   \n",
       "29932  [('morethanjustbones', 'n'), ('seems', 'v'), (...   \n",
       "29933  [('big', 'a'), ('thanks', 'n'), ('sent', 'v'),...   \n",
       "\n",
       "                                lemma_sentence(with POS)  sentiword_analysis  \\\n",
       "0        parliament covid vaccine day not week challe...              -0.625   \n",
       "1            first dose vaccination syringe combat covid              -0.125   \n",
       "2        time forget covid brexit trump sleep listeni...               0.250   \n",
       "3        rachel absolutely nail tonight throughout pr...               1.375   \n",
       "4        kid never say not wana homework get sparkle ...              -0.750   \n",
       "...                                                  ...                 ...   \n",
       "29929    health rover offering rapid covid antigen te...               0.750   \n",
       "29930    americaorbust fox complicit manet death due ...              -0.625   \n",
       "29931    plan defeat covid backhand index point down ...              -0.125   \n",
       "29932    morethanjustbones seem more relevant ever fe...               0.000   \n",
       "29933    big thanks send tonight send morale key work...              -1.250   \n",
       "\n",
       "       vader_score  textblob_polarity  senti_textblob  senti_wordnet  \\\n",
       "0          -0.0572           0.000000               0             -1   \n",
       "1          -0.3400           0.250000               1             -1   \n",
       "2          -0.2263           0.000000               0              1   \n",
       "3           0.6124           0.300000               1              1   \n",
       "4           0.6573           0.025000               1             -1   \n",
       "...            ...                ...             ...            ...   \n",
       "29929       0.0000           0.000000               0              1   \n",
       "29930      -0.8126          -0.125000              -1             -1   \n",
       "29931      -0.6369           0.122222               1             -1   \n",
       "29932       0.6597           0.415000               1              0   \n",
       "29933       0.4767          -0.018333              -1             -1   \n",
       "\n",
       "       senti_vader  \n",
       "0               -1  \n",
       "1               -1  \n",
       "2               -1  \n",
       "3                1  \n",
       "4                1  \n",
       "...            ...  \n",
       "29929            0  \n",
       "29930           -1  \n",
       "29931           -1  \n",
       "29932            1  \n",
       "29933            1  \n",
       "\n",
       "[29934 rows x 19 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('supervised_sample_datasets/lexicon_step1.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample data\n",
    "positive_df = df[df[\"senti_vader\"] == 1]\n",
    "positive_df = positive_df[:1000] #1000 positive sentiment\n",
    "neutral_df = df[df[\"senti_vader\"] == 0]\n",
    "neutral_df = neutral_df[:1000] #1000 neutral sentiment\n",
    "negative_df = df[df[\"senti_vader\"] == -1]\n",
    "negative_df = negative_df[:1000] #1000 neutral sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=[positive_df, neutral_df, negative_df]\n",
    "# df=pd.concat(df)\n",
    "# df=df.reset_index(drop=True)\n",
    "# df.to_csv('supervised_sample_datasets/sample_data.csv', index=False, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after manual check\n",
    "df = pd.read_csv('supervised_sample_datasets/sample_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative: 996\n",
      "neutral 1009\n",
      "positive 995\n"
     ]
    }
   ],
   "source": [
    "#after manual check\n",
    "negative_num=len(df[df['senti_vader'] < 0])\n",
    "print(\"negative:\", negative_num)\n",
    "neutral_num=len(df[df['senti_vader'] == 0])\n",
    "print(\"neutral\", neutral_num)\n",
    "positive_num=len(df[df['senti_vader'] > 0])\n",
    "print(\"positive\", positive_num) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 7363)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BoW\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def bag_of_words(df):\n",
    "#     bow_vectorizer = CountVectorizer(max_df=0.90, min_df=0.2, stop_words=None, tokenizer=word_tokenize) \n",
    "    bow_vectorizer = CountVectorizer() \n",
    "    bow = bow_vectorizer.fit_transform(df['lemma_sentence(with POS)']) \n",
    "    #print(bow_vectorizer.get_feature_names())\n",
    "    #print(bow_vectorizer.vocabulary_)\n",
    "    return bow\n",
    "\n",
    "df_bow=bag_of_words(df)\n",
    "df_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 7363)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF\n",
    "def tf_idf(df):\n",
    "#     tf_idf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.2, stop_words=None, tokenizer=word_tokenize, norm='l2') \n",
    "    tf_idf_vectorizer = TfidfVectorizer(norm='l2') #extract features\n",
    "    tfidf = tf_idf_vectorizer.fit_transform(df['lemma_sentence(with POS)']) #vectors\n",
    "    return tfidf\n",
    "df_tfidf=tf_idf(df)\n",
    "df_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [rachel, absolutely, nail, tonight, throughout...\n",
      "1       [kid, never, say, not, wana, homework, get, sp...\n",
      "2       [not, mess, covid, wear, mask, time, london, u...\n",
      "3       [problem, think, airfield, cost, remain, open,...\n",
      "4       [remain, astonished, stock, market, not, much,...\n",
      "                              ...                        \n",
      "2995    [woke, today, ping, bank, notify, hospital, de...\n",
      "2996    [football, ban, till, covid, control, think, e...\n",
      "2997    [useless, company, affect, financially, cause,...\n",
      "2998    [listen, covid, death, high, people, learn, di...\n",
      "2999    [friend, live, dubai, call, yesterday, say, si...\n",
      "Name: lemma_sentence(with POS), Length: 3000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Word2vec\n",
    "#reference：https://www.pythonf.cn/read/93491\n",
    "#https://github.com/Shwetago/Sentiment_Analysis/blob/master/Twitter_Sentiment_Analysis.ipynb\n",
    "from nltk.tokenize import word_tokenize\n",
    "Tokenize_tweet = df['lemma_sentence(with POS)'].apply(word_tokenize)\n",
    "print(Tokenize_tweet)\n",
    "\n",
    "Model_W2V = gensim.models.Word2Vec(Tokenize_tweet, size=200, #features\n",
    "                                   window=5, \n",
    "                                   min_count=1, \n",
    "                                   sg=1,  #skip-gram model\n",
    "                                   hs=0,\n",
    "                                   negative=10, \n",
    "                                   workers=2, \n",
    "                                   seed=34 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.046028</td>\n",
       "      <td>0.013125</td>\n",
       "      <td>-0.215196</td>\n",
       "      <td>0.054006</td>\n",
       "      <td>-0.045560</td>\n",
       "      <td>0.046852</td>\n",
       "      <td>0.008420</td>\n",
       "      <td>0.032719</td>\n",
       "      <td>-0.166786</td>\n",
       "      <td>-0.141116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027627</td>\n",
       "      <td>0.169176</td>\n",
       "      <td>-0.012951</td>\n",
       "      <td>-0.070157</td>\n",
       "      <td>0.261464</td>\n",
       "      <td>0.042586</td>\n",
       "      <td>0.028671</td>\n",
       "      <td>0.163418</td>\n",
       "      <td>-0.131032</td>\n",
       "      <td>0.012715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.057419</td>\n",
       "      <td>-0.007375</td>\n",
       "      <td>-0.220125</td>\n",
       "      <td>0.045612</td>\n",
       "      <td>-0.039521</td>\n",
       "      <td>0.026838</td>\n",
       "      <td>-0.015003</td>\n",
       "      <td>0.033050</td>\n",
       "      <td>-0.163985</td>\n",
       "      <td>-0.140885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043693</td>\n",
       "      <td>0.178724</td>\n",
       "      <td>-0.029015</td>\n",
       "      <td>-0.078524</td>\n",
       "      <td>0.260154</td>\n",
       "      <td>0.032928</td>\n",
       "      <td>0.034161</td>\n",
       "      <td>0.176410</td>\n",
       "      <td>-0.137545</td>\n",
       "      <td>0.034150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.062423</td>\n",
       "      <td>-0.006053</td>\n",
       "      <td>-0.242087</td>\n",
       "      <td>0.033288</td>\n",
       "      <td>-0.054310</td>\n",
       "      <td>0.037971</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>0.040159</td>\n",
       "      <td>-0.163437</td>\n",
       "      <td>-0.135993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030736</td>\n",
       "      <td>0.196103</td>\n",
       "      <td>-0.033956</td>\n",
       "      <td>-0.082589</td>\n",
       "      <td>0.266228</td>\n",
       "      <td>0.046577</td>\n",
       "      <td>0.028694</td>\n",
       "      <td>0.178976</td>\n",
       "      <td>-0.131259</td>\n",
       "      <td>0.034813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.052720</td>\n",
       "      <td>-0.001593</td>\n",
       "      <td>-0.214062</td>\n",
       "      <td>0.045373</td>\n",
       "      <td>-0.039554</td>\n",
       "      <td>0.033358</td>\n",
       "      <td>-0.009572</td>\n",
       "      <td>0.030766</td>\n",
       "      <td>-0.158312</td>\n",
       "      <td>-0.134943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037588</td>\n",
       "      <td>0.169993</td>\n",
       "      <td>-0.022311</td>\n",
       "      <td>-0.074826</td>\n",
       "      <td>0.250775</td>\n",
       "      <td>0.032766</td>\n",
       "      <td>0.032073</td>\n",
       "      <td>0.168943</td>\n",
       "      <td>-0.132774</td>\n",
       "      <td>0.027918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.061093</td>\n",
       "      <td>-0.013015</td>\n",
       "      <td>-0.227416</td>\n",
       "      <td>0.034307</td>\n",
       "      <td>-0.040996</td>\n",
       "      <td>0.023911</td>\n",
       "      <td>-0.020089</td>\n",
       "      <td>0.036286</td>\n",
       "      <td>-0.158784</td>\n",
       "      <td>-0.134447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043168</td>\n",
       "      <td>0.185263</td>\n",
       "      <td>-0.036659</td>\n",
       "      <td>-0.080655</td>\n",
       "      <td>0.258209</td>\n",
       "      <td>0.030195</td>\n",
       "      <td>0.034023</td>\n",
       "      <td>0.175576</td>\n",
       "      <td>-0.133142</td>\n",
       "      <td>0.035248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>0.061576</td>\n",
       "      <td>-0.015965</td>\n",
       "      <td>-0.209558</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>-0.036717</td>\n",
       "      <td>0.015143</td>\n",
       "      <td>-0.021524</td>\n",
       "      <td>0.034016</td>\n",
       "      <td>-0.156649</td>\n",
       "      <td>-0.125417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046313</td>\n",
       "      <td>0.168563</td>\n",
       "      <td>-0.029884</td>\n",
       "      <td>-0.079577</td>\n",
       "      <td>0.242419</td>\n",
       "      <td>0.033425</td>\n",
       "      <td>0.036464</td>\n",
       "      <td>0.166718</td>\n",
       "      <td>-0.132280</td>\n",
       "      <td>0.039234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>0.057106</td>\n",
       "      <td>-0.005136</td>\n",
       "      <td>-0.229884</td>\n",
       "      <td>0.045231</td>\n",
       "      <td>-0.041574</td>\n",
       "      <td>0.029140</td>\n",
       "      <td>-0.013771</td>\n",
       "      <td>0.032535</td>\n",
       "      <td>-0.169214</td>\n",
       "      <td>-0.144692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042866</td>\n",
       "      <td>0.183323</td>\n",
       "      <td>-0.028413</td>\n",
       "      <td>-0.080946</td>\n",
       "      <td>0.268426</td>\n",
       "      <td>0.035118</td>\n",
       "      <td>0.033613</td>\n",
       "      <td>0.180557</td>\n",
       "      <td>-0.144866</td>\n",
       "      <td>0.032142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>0.069067</td>\n",
       "      <td>-0.019276</td>\n",
       "      <td>-0.233933</td>\n",
       "      <td>0.041691</td>\n",
       "      <td>-0.038708</td>\n",
       "      <td>0.017015</td>\n",
       "      <td>-0.027743</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>-0.172945</td>\n",
       "      <td>-0.144220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055926</td>\n",
       "      <td>0.192329</td>\n",
       "      <td>-0.037111</td>\n",
       "      <td>-0.088606</td>\n",
       "      <td>0.270958</td>\n",
       "      <td>0.031635</td>\n",
       "      <td>0.039046</td>\n",
       "      <td>0.194003</td>\n",
       "      <td>-0.146554</td>\n",
       "      <td>0.047267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>0.059230</td>\n",
       "      <td>-0.010724</td>\n",
       "      <td>-0.206712</td>\n",
       "      <td>0.040984</td>\n",
       "      <td>-0.036715</td>\n",
       "      <td>0.019315</td>\n",
       "      <td>-0.018032</td>\n",
       "      <td>0.033516</td>\n",
       "      <td>-0.154064</td>\n",
       "      <td>-0.129493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044305</td>\n",
       "      <td>0.165126</td>\n",
       "      <td>-0.029433</td>\n",
       "      <td>-0.075435</td>\n",
       "      <td>0.244689</td>\n",
       "      <td>0.036557</td>\n",
       "      <td>0.034476</td>\n",
       "      <td>0.163093</td>\n",
       "      <td>-0.133008</td>\n",
       "      <td>0.033050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>0.059337</td>\n",
       "      <td>-0.004589</td>\n",
       "      <td>-0.225224</td>\n",
       "      <td>0.045626</td>\n",
       "      <td>-0.041885</td>\n",
       "      <td>0.028225</td>\n",
       "      <td>-0.012095</td>\n",
       "      <td>0.033453</td>\n",
       "      <td>-0.167216</td>\n",
       "      <td>-0.142214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042537</td>\n",
       "      <td>0.179374</td>\n",
       "      <td>-0.028606</td>\n",
       "      <td>-0.079372</td>\n",
       "      <td>0.265059</td>\n",
       "      <td>0.037705</td>\n",
       "      <td>0.034430</td>\n",
       "      <td>0.176696</td>\n",
       "      <td>-0.143265</td>\n",
       "      <td>0.031745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     0.046028  0.013125 -0.215196  0.054006 -0.045560  0.046852  0.008420   \n",
       "1     0.057419 -0.007375 -0.220125  0.045612 -0.039521  0.026838 -0.015003   \n",
       "2     0.062423 -0.006053 -0.242087  0.033288 -0.054310  0.037971 -0.011595   \n",
       "3     0.052720 -0.001593 -0.214062  0.045373 -0.039554  0.033358 -0.009572   \n",
       "4     0.061093 -0.013015 -0.227416  0.034307 -0.040996  0.023911 -0.020089   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2995  0.061576 -0.015965 -0.209558  0.039427 -0.036717  0.015143 -0.021524   \n",
       "2996  0.057106 -0.005136 -0.229884  0.045231 -0.041574  0.029140 -0.013771   \n",
       "2997  0.069067 -0.019276 -0.233933  0.041691 -0.038708  0.017015 -0.027743   \n",
       "2998  0.059230 -0.010724 -0.206712  0.040984 -0.036715  0.019315 -0.018032   \n",
       "2999  0.059337 -0.004589 -0.225224  0.045626 -0.041885  0.028225 -0.012095   \n",
       "\n",
       "           7         8         9    ...       190       191       192  \\\n",
       "0     0.032719 -0.166786 -0.141116  ...  0.027627  0.169176 -0.012951   \n",
       "1     0.033050 -0.163985 -0.140885  ...  0.043693  0.178724 -0.029015   \n",
       "2     0.040159 -0.163437 -0.135993  ...  0.030736  0.196103 -0.033956   \n",
       "3     0.030766 -0.158312 -0.134943  ...  0.037588  0.169993 -0.022311   \n",
       "4     0.036286 -0.158784 -0.134447  ...  0.043168  0.185263 -0.036659   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2995  0.034016 -0.156649 -0.125417  ...  0.046313  0.168563 -0.029884   \n",
       "2996  0.032535 -0.169214 -0.144692  ...  0.042866  0.183323 -0.028413   \n",
       "2997  0.033497 -0.172945 -0.144220  ...  0.055926  0.192329 -0.037111   \n",
       "2998  0.033516 -0.154064 -0.129493  ...  0.044305  0.165126 -0.029433   \n",
       "2999  0.033453 -0.167216 -0.142214  ...  0.042537  0.179374 -0.028606   \n",
       "\n",
       "           193       194       195       196       197       198       199  \n",
       "0    -0.070157  0.261464  0.042586  0.028671  0.163418 -0.131032  0.012715  \n",
       "1    -0.078524  0.260154  0.032928  0.034161  0.176410 -0.137545  0.034150  \n",
       "2    -0.082589  0.266228  0.046577  0.028694  0.178976 -0.131259  0.034813  \n",
       "3    -0.074826  0.250775  0.032766  0.032073  0.168943 -0.132774  0.027918  \n",
       "4    -0.080655  0.258209  0.030195  0.034023  0.175576 -0.133142  0.035248  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2995 -0.079577  0.242419  0.033425  0.036464  0.166718 -0.132280  0.039234  \n",
       "2996 -0.080946  0.268426  0.035118  0.033613  0.180557 -0.144866  0.032142  \n",
       "2997 -0.088606  0.270958  0.031635  0.039046  0.194003 -0.146554  0.047267  \n",
       "2998 -0.075435  0.244689  0.036557  0.034476  0.163093 -0.133008  0.033050  \n",
       "2999 -0.079372  0.265059  0.037705  0.034430  0.176696 -0.143265  0.031745  \n",
       "\n",
       "[3000 rows x 200 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Each word can get its own vector. The representation of a tweets can the vector sum of each word divided by the total number(average) \n",
    "#or just the sum of each word vector\n",
    "def word2vec_tweet(tokens, size):\n",
    "    vector=np.zeros(size).reshape((1,size))\n",
    "    vector_cnt = 0\n",
    "    for word in tokens:\n",
    "        vector += Model_W2V[word].reshape((1, size))\n",
    "        vector_cnt += 1\n",
    "    return vector/vector_cnt  #average for tweets\n",
    "\n",
    "def word2vec_tweet_2(tokens, size):\n",
    "    vector=np.zeros(size).reshape((1,size))\n",
    "    vector_cnt = 0\n",
    "    for word in tokens:\n",
    "        vector += Model_W2V[word].reshape((1, size))\n",
    "    return vector  #sum of tweets\n",
    "\n",
    "tweet_arr=np.zeros((len(Tokenize_tweet), 200))\n",
    "\n",
    "for i in range (len(Tokenize_tweet)):\n",
    "    tweet_arr[i,:] = word2vec_tweet(Tokenize_tweet[i], 200)\n",
    "tweet_vec_df = pd.DataFrame(tweet_arr)\n",
    "tweet_vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW for three classification models\n",
    "#split the train and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_bow, df['senti_textblob'],test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [10, 20, 30, 40], 'min_samples_split': [2, 5, 10, 15], 'min_samples_leaf': [1, 2, 5, 10]}\n"
     ]
    }
   ],
   "source": [
    "#parameters in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 20)] #tree number\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "max_depth = [10,20,30,40]\n",
    "min_samples_split = [2, 5, 10, 15]\n",
    "min_samples_leaf = [1, 2, 5, 10]\n",
    "\n",
    "# Create the param grid\n",
    "param_grid_forest = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "print(param_grid_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [0.01, 0.1, 0.5, 1.0, 5.0, 10.0], 'fit_prior': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "#parameters in MNB\n",
    "param_grid_nb = {'alpha': [0.01, 0.1, 0.5, 1.0, 5.0, 10.0],\n",
    "                'fit_prior':[True, False]}\n",
    "print(param_grid_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': [1, 10, 100, 1000], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'degree': [1, 2, 3, 4]}\n"
     ]
    }
   ],
   "source": [
    "#parameters in SVC\n",
    "# c_list=list(range(1,51))\n",
    "param_grid_svc = {'C': [1, 10, 100, 1000],\n",
    "                  'kernel': ['linear','poly','rbf','sigmoid'],\n",
    "                  'degree': [1,2,3,4]}\n",
    "print(param_grid_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble._forest import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model_forest = RandomForestClassifier()\n",
    "model_nb = MultinomialNB()\n",
    "model_svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 140,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 40}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best parameters for RF(with BoW)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "RF_RandomGrid = RandomizedSearchCV(estimator = model_forest, param_distributions = param_grid_forest, cv = 10, verbose=2, n_jobs = 4)\n",
    "RF_RandomGrid.fit(x_train, y_train)\n",
    "RF_RandomGrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_prior': False, 'alpha': 1.0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best parameters for MNB(with BoW)\n",
    "NB_RandomGrid = RandomizedSearchCV(estimator = model_nb, param_distributions = param_grid_nb, cv = 10, verbose=2, n_jobs = 4)\n",
    "NB_RandomGrid.fit(x_train, y_train)\n",
    "NB_RandomGrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kernel': 'linear', 'degree': 1, 'C': 1}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best parameters for SVC(with BoW)\n",
    "SVC_RandomGrid = RandomizedSearchCV(estimator = model_svc, param_distributions = param_grid_svc, cv = 10, verbose=2, n_jobs = 4)\n",
    "SVC_RandomGrid.fit(x_train, y_train)\n",
    "SVC_RandomGrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.95      0.25      0.40       144\n",
      "           0       0.80      0.70      0.74       184\n",
      "           1       0.64      0.94      0.76       272\n",
      "\n",
      "    accuracy                           0.70       600\n",
      "   macro avg       0.79      0.63      0.63       600\n",
      "weighted avg       0.76      0.70      0.67       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model establishment and results(BoW)\n",
    "#Random Forest\n",
    "model_forest = RandomForestClassifier(n_estimators=140,min_samples_split=10, min_samples_leaf=2, max_features='auto', max_depth=40)\n",
    "model_forest.fit(x_train,y_train)\n",
    "prediction = model_forest.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import classification_report \n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.52      0.47      0.50       144\n",
      "           0       0.76      0.53      0.62       184\n",
      "           1       0.62      0.78      0.69       272\n",
      "\n",
      "    accuracy                           0.63       600\n",
      "   macro avg       0.63      0.59      0.60       600\n",
      "weighted avg       0.64      0.63      0.62       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_nb = MultinomialNB(alpha=1.0, fit_prior=False)\n",
    "model_nb = model_nb.fit(x_train,y_train)\n",
    "prediction_nb = model_nb.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, prediction_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.60      0.59      0.59       144\n",
      "           0       0.70      0.79      0.74       184\n",
      "           1       0.79      0.73      0.76       272\n",
      "\n",
      "    accuracy                           0.71       600\n",
      "   macro avg       0.70      0.70      0.70       600\n",
      "weighted avg       0.72      0.71      0.71       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_svc = SVC(kernel='linear',degree=1, C=1)\n",
    "model_svc = model_svc.fit(x_train,y_train)\n",
    "prediction_svc = model_svc.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, prediction_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(df_tfidf, df['senti_textblob'],test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble._forest import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model_forest = RandomForestClassifier()\n",
    "model_nb = MultinomialNB()\n",
    "model_svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 120,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 40}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best parameters for RF(with TFIDF)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "RF_RandomGrid = RandomizedSearchCV(estimator = model_forest, param_distributions = param_grid_forest, cv = 10, verbose=2, n_jobs = 4)\n",
    "RF_RandomGrid.fit(x_train_2, y_train_2)\n",
    "RF_RandomGrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_prior': False, 'alpha': 0.5}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_RandomGrid = RandomizedSearchCV(estimator = model_nb, param_distributions = param_grid_nb, cv = 10, verbose=2, n_jobs = 4)\n",
    "NB_RandomGrid.fit(x_train_2, y_train_2)\n",
    "NB_RandomGrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kernel': 'linear', 'degree': 3, 'C': 10}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC_RandomGrid = RandomizedSearchCV(estimator = model_svc, param_distributions = param_grid_svc, cv = 10, verbose=2, n_jobs = 4)\n",
    "SVC_RandomGrid.fit(x_train_2, y_train_2)\n",
    "SVC_RandomGrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.87      0.19      0.31       145\n",
      "           0       0.73      0.63      0.68       178\n",
      "           1       0.61      0.92      0.73       277\n",
      "\n",
      "    accuracy                           0.66       600\n",
      "   macro avg       0.74      0.58      0.57       600\n",
      "weighted avg       0.71      0.66      0.62       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model establishment and results(TF-IDF)\n",
    "#Random Forest\n",
    "model_forest = RandomForestClassifier(n_estimators=120,min_samples_split=5, min_samples_leaf=2, max_features='auto', max_depth=40)\n",
    "#model_forest = RandomForestClassifier()\n",
    "model_forest.fit(x_train_2,y_train_2)\n",
    "prediction = model_forest.predict(x_test_2)\n",
    "\n",
    "from sklearn.metrics import classification_report \n",
    "print(classification_report(y_test_2, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.50      0.48      0.49       145\n",
      "           0       0.70      0.49      0.58       178\n",
      "           1       0.64      0.77      0.70       277\n",
      "\n",
      "    accuracy                           0.62       600\n",
      "   macro avg       0.61      0.58      0.59       600\n",
      "weighted avg       0.62      0.62      0.61       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_nb = MultinomialNB(alpha=0.5, fit_prior=False)\n",
    "model_nb = model_nb.fit(x_train_2,y_train_2)\n",
    "prediction_nb = model_nb.predict(x_test_2)\n",
    "\n",
    "print(classification_report(y_test_2, prediction_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.58      0.60      0.59       145\n",
      "           0       0.72      0.78      0.75       178\n",
      "           1       0.77      0.71      0.74       277\n",
      "\n",
      "    accuracy                           0.71       600\n",
      "   macro avg       0.69      0.70      0.69       600\n",
      "weighted avg       0.71      0.71      0.71       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_svc = SVC(C=10, kernel='linear',degree=3)\n",
    "model_svc = model_svc.fit(x_train_2,y_train_2)\n",
    "prediction_svc = model_svc.predict(x_test_2)\n",
    "\n",
    "print(classification_report(y_test_2, prediction_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2vec\n",
    "x_train_3, x_test_3, y_train_3, y_test_3 = train_test_split(tweet_vec_df, df['senti_textblob'],test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble._forest import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model_forest = RandomForestClassifier()\n",
    "model_nb = MultinomialNB()\n",
    "model_svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 160,\n",
       " 'min_samples_split': 15,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'log2',\n",
       " 'max_depth': 30}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best parameters for RF(with Word2vec)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "RF_RandomGrid = RandomizedSearchCV(estimator = model_forest, param_distributions = param_grid_forest, cv = 10, verbose=2, n_jobs = 4)\n",
    "RF_RandomGrid.fit(x_train_3, y_train_3)\n",
    "RF_RandomGrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kernel': 'linear', 'degree': 2, 'C': 1000}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC_RandomGrid = RandomizedSearchCV(estimator = model_svc, param_distributions = param_grid_svc, cv = 10, verbose=2, n_jobs = 4)\n",
    "SVC_RandomGrid.fit(x_train_3, y_train_3)\n",
    "SVC_RandomGrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_prior': True, 'alpha': 10.0}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() #handle negative\n",
    "x_train_3a = scaler.fit_transform(x_train_3)\n",
    "x_test_3a = scaler.fit_transform(x_test_3)\n",
    "NB_RandomGrid = RandomizedSearchCV(estimator = model_nb, param_distributions = param_grid_nb, cv = 10, verbose=2, n_jobs = 4)\n",
    "NB_RandomGrid.fit(x_train_3a, y_train_3)\n",
    "NB_RandomGrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.47      0.12      0.19       152\n",
      "           0       0.61      0.47      0.53       194\n",
      "           1       0.51      0.82      0.63       254\n",
      "\n",
      "    accuracy                           0.53       600\n",
      "   macro avg       0.53      0.47      0.45       600\n",
      "weighted avg       0.53      0.53      0.49       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model establishment and results(Word2Vec)\n",
    "#Random Forest\n",
    "model_forest = RandomForestClassifier(n_estimators=160,min_samples_split=15, min_samples_leaf=2, max_features='log2', max_depth=30)\n",
    "model_forest.fit(x_train_3,y_train_3)\n",
    "prediction = model_forest.predict(x_test_3)\n",
    "\n",
    "from sklearn.metrics import classification_report \n",
    "print(classification_report(y_test_3, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.75      0.06      0.11       152\n",
      "           0       0.71      0.50      0.59       194\n",
      "           1       0.52      0.92      0.66       254\n",
      "\n",
      "    accuracy                           0.56       600\n",
      "   macro avg       0.66      0.49      0.45       600\n",
      "weighted avg       0.64      0.56      0.50       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_svc = SVC(C=1000, kernel='linear', degree=2)\n",
    "model_svc = model_svc.fit(x_train_3,y_train_3)\n",
    "prediction_svc = model_svc.predict(x_test_3)\n",
    "\n",
    "print(classification_report(y_test_3, prediction_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.27      0.27      0.27       152\n",
      "           0       0.56      0.28      0.38       194\n",
      "           1       0.46      0.63      0.53       254\n",
      "\n",
      "    accuracy                           0.43       600\n",
      "   macro avg       0.43      0.40      0.39       600\n",
      "weighted avg       0.44      0.43      0.42       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# x_train_3 = scaler.fit_transform(x_train_3)\n",
    "# x_test_3 = scaler.fit_transform(x_test_3)\n",
    "\n",
    "model_nb = MultinomialNB(alpha=10, fit_prior=False)\n",
    "model_nb = model_nb.fit(x_train_3a,y_train_3)\n",
    "prediction_nb = model_nb.predict(x_test_3a)\n",
    "\n",
    "print(classification_report(y_test_3, prediction_nb)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
