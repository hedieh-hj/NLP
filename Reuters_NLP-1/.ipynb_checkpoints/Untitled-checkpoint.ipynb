{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ff5bbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import bigrams, FreqDist\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3691f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50d88f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.']\n"
     ]
    }
   ],
   "source": [
    "# get words\n",
    "words = reuters.words()\n",
    "\n",
    "print(\"Original Words:\", words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78b31d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Removing Special Characters: ['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', 'S', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', 'S', 'And', 'Japan', 'has', 'raised']\n"
     ]
    }
   ],
   "source": [
    "# delete non alphabetic(numbers, sign)\n",
    "words = [word for word in words if word.isalpha()]\n",
    "\n",
    "print(\"After Removing Special Characters:\", words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a866052a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95edcb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Removing Stop Words: ['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'U', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'U', 'Japan', 'raised', 'fears', 'among', 'many', 'Asia', 'exporting', 'nations', 'row']\n"
     ]
    }
   ],
   "source": [
    "# delete stop words\n",
    "# Stop words are like \"is\",\"the\" ...\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(\"After Removing Stop Words:\", words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fc566bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Stemming: ['asian', 'export', 'fear', 'damag', 'u', 'japan', 'rift', 'mount', 'trade', 'friction', 'u', 'japan', 'rais', 'fear', 'among', 'mani', 'asia', 'export', 'nation', 'row']\n"
     ]
    }
   ],
   "source": [
    "# stemming --> warning: warn \n",
    "stemmer = PorterStemmer()\n",
    "words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(\"After Stemming:\", words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a070285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37184feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c9e9803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Tokenization: ['asian', 'export', 'fear', 'damag', 'u', 'japan', 'rift', 'mount', 'trade', 'friction', 'u', 'japan', 'rais', 'fear', 'among', 'mani', 'asia', 'export', 'nation', 'row']\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "tokens = word_tokenize(\" \".join(words))\n",
    "\n",
    "print(\"After Tokenization:\", tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b97cea4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['This', 'is', 'a', 'sample', 'sentence', 'for', 'tokenization', '.']\n",
      "After Tokenization: ['This', 'is', 'a', 'sample', 'sentence', 'for', 'tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------- test word_tokenize\n",
    "# sentence = \"This is a sample sentence for tokenization.\"\n",
    "# tokens = word_tokenize(sentence, language=\"english\", preserve_line=False)\n",
    "# print(\"Tokenized Sentence:\", tokens)\n",
    "\n",
    "# print(\"After Tokenization:\", tokens[:20])\n",
    "# ---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc695048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: [('asian', 'export'), ('export', 'fear'), ('fear', 'damag'), ('damag', 'u'), ('u', 'japan'), ('japan', 'rift'), ('rift', 'mount'), ('mount', 'trade'), ('trade', 'friction'), ('friction', 'u')]\n"
     ]
    }
   ],
   "source": [
    "# calculate bigrams (w1,w2)\n",
    "from nltk import bigrams\n",
    "\n",
    "bigrams_list = list(bigrams(words))\n",
    "print(\"Bigrams:\", bigrams_list[:10])  # Show first 10 bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a4ce8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Bigrams: [(('mln', 'dlr'), 4838), (('vs', 'mln'), 3947), (('mln', 'vs'), 3921), (('ct', 'vs'), 3386), (('ct', 'net'), 2247), (('vs', 'ct'), 1934), (('billion', 'dlr'), 1914), (('vs', 'loss'), 1785), (('rev', 'mln'), 1619), (('net', 'vs'), 1579), (('shr', 'ct'), 1480), (('inc', 'lt'), 1401), (('compani', 'said'), 1379), (('last', 'year'), 1368), (('dlr', 'vs'), 1261), (('corp', 'lt'), 1129), (('net', 'shr'), 1079), (('avg', 'shr'), 1055), (('per', 'share'), 1028), (('vs', 'rev'), 1002), (('vs', 'profit'), 997), (('net', 'loss'), 955), (('loss', 'ct'), 919), (('inc', 'said'), 913), (('mln', 'stg'), 899), (('shr', 'loss'), 874), (('qtr', 'net'), 829), (('mln', 'tonn'), 827), (('mln', 'note'), 780), (('vs', 'dlr'), 770), (('net', 'profit'), 760), (('offici', 'said'), 754), (('dlr', 'per'), 730), (('corp', 'said'), 724), (('sale', 'mln'), 687), (('dlr', 'net'), 680), (('sourc', 'said'), 677), (('shr', 'vs'), 671), (('loss', 'vs'), 668), (('nine', 'mth'), 658), (('dlr', 'share'), 655), (('net', 'mln'), 653), (('central', 'bank'), 641), (('mln', 'avg'), 641), (('oper', 'net'), 633), (('unit', 'state'), 629), (('u', 'k'), 625), (('interest', 'rate'), 607), (('vs', 'billion'), 603), (('also', 'said'), 588), (('mth', 'shr'), 588), (('loss', 'dlr'), 585), (('loss', 'mln'), 581), (('rev', 'vs'), 571), (('first', 'quarter'), 566), (('billion', 'vs'), 560), (('shr', 'dlr'), 554), (('oper', 'shr'), 540), (('loss', 'rev'), 536), (('ct', 'share'), 533), (('profit', 'ct'), 516), (('shr', 'profit'), 515), (('analyst', 'said'), 514), (('spokesman', 'said'), 506), (('year', 'ago'), 502), (('year', 'shr'), 499), (('rose', 'pct'), 498), (('mln', 'rev'), 489), (('profit', 'vs'), 485), (('new', 'york'), 482), (('pct', 'pct'), 477), (('common', 'stock'), 476), (('last', 'month'), 475), (('said', 'expect'), 472), (('ct', 'prior'), 459), (('last', 'week'), 457), (('dlr', 'ct'), 448), (('profit', 'mln'), 445), (('told', 'reuter'), 443), (('qtli', 'div'), 442), (('money', 'market'), 441), (('mln', 'year'), 433), (('ct', 'per'), 432), (('exchang', 'rate'), 423), (('year', 'end'), 414), (('vs', 'sale'), 407), (('shr', 'mln'), 407), (('said', 'said'), 406), (('depart', 'said'), 406), (('vs', 'note'), 403), (('dealer', 'said'), 394), (('ct', 'oper'), 391), (('oil', 'price'), 387), (('dlr', 'dlr'), 384), (('crude', 'oil'), 381), (('year', 'earlier'), 379), (('div', 'ct'), 373), (('foreign', 'exchang'), 370), (('year', 'net'), 370), (('said', 'would'), 368)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "bigram_freq = FreqDist(bigrams_list)\n",
    "print(\"Most Common Bigrams:\", bigram_freq.most_common(100))  # Show 10 most frequent bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcb7c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
