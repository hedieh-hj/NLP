{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f61a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import stanza\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94fd51ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Sentiment140 dataset (replace the path with your actual file path)\n",
    "df = pd.read_csv('sentiment140.csv', encoding='ISO-8859-1', header=None)\n",
    "df.columns = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "# Preview the dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42d5d5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a thats a bummer  you shoulda got david car...</td>\n",
       "      <td>[a, thats, a, bummer, you, shoulda, got, david...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>[is, upset, that, he, cant, update, his, faceb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i dived many times for the ball managed to sa...</td>\n",
       "      <td>[i, dived, many, times, for, the, ball, manage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no its not behaving at all im mad why am i he...</td>\n",
       "      <td>[no, its, not, behaving, at, all, im, mad, why...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text  \\\n",
       "0     a thats a bummer  you shoulda got david car...   \n",
       "1  is upset that he cant update his facebook by t...   \n",
       "2   i dived many times for the ball managed to sa...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4   no its not behaving at all im mad why am i he...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [a, thats, a, bummer, you, shoulda, got, david...  \n",
       "1  [is, upset, that, he, cant, update, his, faceb...  \n",
       "2  [i, dived, many, times, for, the, ball, manage...  \n",
       "3  [my, whole, body, feels, itchy, and, like, its...  \n",
       "4  [no, its, not, behaving, at, all, im, mad, why...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Basic preprocessing function\n",
    "def preprocess(text):\n",
    "    # Remove links, user handles, and non-alphabetic characters\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)  # Remove non-alphabetic characters\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned_text'] = df['text'].apply(preprocess)\n",
    "\n",
    "# Tokenize text for word2vec input\n",
    "df['tokens'] = df['cleaned_text'].apply(word_tokenize)\n",
    "df[['cleaned_text', 'tokens']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a303803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.1380569e+00,  1.6798391e+00,  5.3490782e-01, -1.3220973e+00,\n",
       "        1.5473249e+00,  2.8470736e+00, -1.0451984e+00,  5.1259956e+00,\n",
       "       -8.8799024e-01,  1.8172508e+00, -1.3332095e+00,  5.9322685e-01,\n",
       "       -9.4213879e-01,  1.8080770e+00, -1.2684439e+00,  9.3826538e-01,\n",
       "        1.5614133e+00,  1.1706752e+00, -4.5590761e-01,  1.4284602e+00,\n",
       "        2.9882237e-01,  2.5660779e+00, -1.4944427e+00,  6.7218417e-01,\n",
       "       -6.2280458e-01,  3.5854176e-01, -1.0778033e+00, -5.8193398e-01,\n",
       "       -4.7605243e+00, -2.0378101e+00,  7.4142665e-01,  1.2382221e+00,\n",
       "        2.6300892e-01,  1.7938393e+00,  4.2192297e+00, -8.1224136e-02,\n",
       "        8.7886113e-01,  2.3239298e+00, -2.8447332e+00,  8.4191501e-01,\n",
       "        1.1227735e+00, -1.0532027e-01, -1.8163227e+00, -6.3161486e-01,\n",
       "       -3.2177576e-01, -6.6026938e-01, -8.0751383e-01,  4.4102979e+00,\n",
       "       -1.9376391e+00,  1.1603693e-02,  1.4293714e+00,  3.2226915e+00,\n",
       "       -1.5676018e+00, -1.8569261e-01, -2.4214728e+00,  2.7329717e+00,\n",
       "       -1.7541644e-01,  3.7989199e-01, -2.5092093e-02,  1.9229237e+00,\n",
       "       -1.2523922e+00, -3.6489969e-01,  3.4764504e+00,  2.3532619e+00,\n",
       "       -4.0697908e-01, -4.1910055e-01,  3.5582316e-01,  1.5629992e-01,\n",
       "        1.4884663e+00,  2.0408328e+00,  3.3628708e-01,  1.1902585e+00,\n",
       "        2.3179376e-03, -9.2019224e-01,  7.6689172e-01,  9.8617017e-01,\n",
       "        5.4347736e-01, -3.6572906e-01, -9.6871358e-01,  2.1066689e+00,\n",
       "        3.3096302e-01,  1.3704331e+00,  6.1986685e-01, -2.2475324e+00,\n",
       "       -3.0299337e+00,  3.5076165e+00,  1.3568357e+00,  8.9798701e-01,\n",
       "        4.1313620e+00, -1.5795964e+00, -3.5196567e-01,  6.8153894e-01,\n",
       "       -1.1443028e+00,  6.9403511e-01,  3.1530070e+00,  3.9538139e-01,\n",
       "        1.6998165e+00,  2.4011545e-01,  4.8996356e-01,  2.0582414e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Word2Vec model\n",
    "sentences = df['tokens'].values.tolist()  # List of tokenized sentences\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the model for later use\n",
    "model.save(\"word2vec_sentiment140.model\")\n",
    "\n",
    "# Check the vector for a specific word\n",
    "model.wv['happy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5772f29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [-0.04507019370794296, 0.303500477806665, 0.48...\n",
       "1    [-0.14101133531048185, 0.7837200789224534, 0.1...\n",
       "2    [0.17004649911541492, -0.8581965663870506, -0....\n",
       "3    [-0.6621622829232365, -0.3362919680774212, -0....\n",
       "4    [-0.10064557606820017, 0.15758222572039812, 0....\n",
       "Name: sentence_embedding, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentence_embedding(tokens, model):\n",
    "    # Create a vector by averaging word vectors for each token in the sentence\n",
    "    vector = np.zeros(100)  # Word2Vec vector size is 100\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        if word in model.wv:\n",
    "            vector += model.wv[word]\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        vector /= count  # Normalize the vector by dividing by the number of words\n",
    "    return vector\n",
    "\n",
    "# Apply sentence embedding to all sentences in the dataset\n",
    "df['sentence_embedding'] = df['tokens'].apply(lambda tokens: sentence_embedding(tokens, model))\n",
    "\n",
    "# Check the first sentence embedding\n",
    "df['sentence_embedding'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5c490f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24811003d49049b695da17df2c07e713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 15:21:18 INFO: Downloaded file to C:\\Users\\PC\\stanza_resources\\resources.json\n",
      "2024-11-15 15:21:18 INFO: Downloading default packages for language: en (English) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060fd79663dc46639934092bc9f315d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.9.0/models/default.zip:   0%|          | 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 15:26:32 INFO: Downloaded file to C:\\Users\\PC\\stanza_resources\\en\\default.zip\n",
      "2024-11-15 15:26:37 INFO: Finished downloading models and saved to C:\\Users\\PC\\stanza_resources\n",
      "2024-11-15 15:26:37 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a9767dbfab49819d1bca9250b00456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 15:26:38 INFO: Downloaded file to C:\\Users\\PC\\stanza_resources\\resources.json\n",
      "2024-11-15 15:26:38 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-11-15 15:26:38 INFO: Using device: cpu\n",
      "2024-11-15 15:26:38 INFO: Loading: tokenize\n",
      "2024-11-15 15:26:38 INFO: Loading: mwt\n",
      "2024-11-15 15:26:38 INFO: Loading: pos\n",
      "2024-11-15 15:26:39 INFO: Loading: lemma\n",
      "2024-11-15 15:26:39 INFO: Loading: depparse\n",
      "2024-11-15 15:26:39 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Download and initialize Stanza for dependency parsing\n",
    "stanza.download('en')  # Download English model for Stanza\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,mwt,pos,lemma,depparse')\n",
    "\n",
    "# Function to parse the sentence and create a dependency graph\n",
    "def parse_dependencies(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    dep_graph = defaultdict(list)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            dep_graph[word.id].append((word.text, word.head))\n",
    "    return dep_graph\n",
    "\n",
    "# Apply the dependency parsing to the cleaned text\n",
    "df['dependencies'] = df['cleaned_text'].apply(parse_dependencies)\n",
    "\n",
    "# Visualize the dependencies of a sentence\n",
    "def visualize_dependency_graph(dep_graph):\n",
    "    G = nx.DiGraph()\n",
    "    for word_id, (word, head) in dep_graph.items():\n",
    "        G.add_edge(head, word_id, label=word)\n",
    "    \n",
    "    # Plot the graph\n",
    "    nx.draw(G, with_labels=True, font_weight='bold', node_color='skyblue', font_size=10, node_size=2000)\n",
    "    plt.show()\n",
    "\n",
    "# Example for the first sentence\n",
    "import networkx as nx\n",
    "visualize_dependency_graph(df['dependencies'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7612ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
